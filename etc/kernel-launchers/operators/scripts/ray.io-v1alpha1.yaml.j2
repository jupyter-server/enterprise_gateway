apiVersion: ray.io/v1alpha1
kind: RayCluster
metadata:
  labels:
    controller-tools.k8s.io: "1.0"
    ray.io/cluster-name: "{{ kernel_resource_name }}"
  annotations:
    ray.io/ft-enabled: "false"  # Disable GCS FT for faster startup
  name: {{ kernel_resource_name }}
spec:
    enableInTreeAutoscaling: true
    autoscalerOptions:
      upscalingMode: Aggressive
      idleTimeoutSeconds: 3600
      imagePullPolicy: Always
      resources:
        limits:
          cpu: 1
          memory: "1Gi"
        requests:
          cpu: 1
          memory: "1Gi"
##########################################
## HEAD Node group spec
##########################################
    headGroupSpec:
      serviceType: ClusterIP # optional
      # the following params are used to complete the ray start: ray start --head --block --port=6379 ...
      rayStartParams:
        disable-usage-stats: 'true'
        dashboard-host: '0.0.0.0'
        block: 'true'
      template:
        metadata:
          labels:
            kernel_id: "{{ kernel_id }}"
            app: enterprise-gateway
            component: kernel
          annotations:
            cluster-autoscaler.kubernetes.io/safe-to-evict: "false"
        spec:
          restartPolicy: OnFailure
          serviceAccountName: "{{ kernel_service_account_name }}"
#          nodeSelector:
#            node.kubernetes.io/instance-type: m5d.8xlarge
          containers:
          # The Ray head container
          - name: ray-head
            image: {{ kernel_image }}
            imagePullPolicy: Always
            securityContext:
              allowPrivilegeEscalation: false
              runAsUser: 0
            resources:
              limits:
                cpu: {{ kernel_head_num_cpu_limit | default(2)}}
                memory: {{ kernel_head_memory_limit | default("4Gi")}}
              requests:
                cpu: {{ kernel_head_num_cpu_request | default(2)}}
                memory: {{ kernel_head_memory_request | default("4Gi")}}
            ports:
            - containerPort: 6379
              name: gcs
            - containerPort: 8000
              name: serve
            - containerPort: 8265
              name: dashboard
            - containerPort: 10001
              name: client
            - containerPort: 44217
              name: as-metrics # autoscaler
            - containerPort: 44227
              name: dash-metrics # dashboard
            startupProbe:
              exec:
                command:
                - /bin/sh
                - -c
                - |
                  # Check GCS port 6379
                  nc -z localhost 6379 || exit 1
                  # Check Ray API cluster status (verifies GCS is ready)
                  wget -q -O- --timeout=5 http://localhost:8265/api/cluster_status 2>/dev/null | grep -q "ALIVE" || exit 1
                  # Check dashboard port 8265
                  wget -q --spider --timeout=30 http://localhost:8265/ || exit 1
              initialDelaySeconds: 15
              periodSeconds: 5
              timeoutSeconds: 5
              failureThreshold: 24
              successThreshold: 1
            readinessProbe:
              exec:
                command:
                - /bin/sh
                - -c
                - |
                  # Check GCS port 6379
                  nc -z localhost 6379 || exit 1
                  # Check Ray API cluster status (verifies GCS is ready)
                  wget -q -O- --timeout=5 http://localhost:8265/api/cluster_status 2>/dev/null | grep -q "ALIVE" || exit 1
                  # Check dashboard port 8265
                  wget -q --spider --timeout=30 http://localhost:8265/ || exit 1
              initialDelaySeconds: 0
              periodSeconds: 10
              timeoutSeconds: 3
              failureThreshold: 3
              successThreshold: 1
            livenessProbe:
              httpGet:
                path: /
                port: 8265
              initialDelaySeconds: 30
              periodSeconds: 20
              timeoutSeconds: 5
              failureThreshold: 3
              successThreshold: 1
          - name: ray-kernel
            image: {{ kernel_image }}
            imagePullPolicy: Always
            securityContext:
              allowPrivilegeEscalation: false
              runAsUser: 0
            env:
              - name: RAY_ADDRESS
                value: "127.0.0.1:6379"
              - name: RAY_PORT
                value: "6379"
              - name: SERVE_URI
                value: "{{ kernel_serve_url }}"
              - name: BUILD_URI
                value: "{{ kernel_build_url }}"
              - name: EG_LOG_LEVEL
                value: "0"
#              - name: PIP_INDEX_URL
#                value: https://pypi.org
            command:
              - "/bin/sh"
              - "-c"
              - "python /usr/local/bin/kernel-launchers/python/scripts/launch_ipykernel.py --kernel-id {{ kernel_id }} --response-address {{ eg_response_address }} --port-range {{ eg_port_range }} --public-key {{ eg_public_key }}"
##########################################
## CPU Workers group specs
##########################################
    workerGroupSpecs:
    - replicas: 1
      minReplicas: 1
      maxReplicas:  {{ kernel_num_cpu_worker or 1 }}
      groupName: cpu-group
      rayStartParams:
        block: 'true'
      template:
        metadata:
          annotations:
           ray.io/compute-image: {{ kernel_image }}
           cluster-autoscaler.kubernetes.io/safe-to-evict: "false"
        spec:
          serviceAccountName: "{{ kernel_service_account_name }}"
#          nodeSelector:
#            node.kubernetes.io/instance-type: {{ kernel_cpu_instance_type | default("m5d.8xlarge")}}
          initContainers:
          - name: init
            image: busybox:1.28
            command: ['sh', '-c', "until nslookup $RAY_IP.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for K8s Service $RAY_IP; sleep 2; done"]
          containers:
          - name: ray-cpu-worker
            image: {{ kernel_image }}
            imagePullPolicy: Always
#            env:
#              - name: PIP_INDEX_URL
#                value: https://pypi.org
            resources:
              limits:
                cpu: {{ kernel_cpu_worker_num_cpu_limit | default(1)}}
                memory: {{ kernel_cpu_worker_num_memory_limit | default("1Gi")}}
              requests:
                cpu: {{ kernel_cpu_worker_num_cpu_request | default("500m")}}
                memory: {{ kernel_cpu_worker_num_memory_request | default("1Gi")}}
#            volumeMounts:
#            - name: ray-logs
#              mountPath: /tmp/ray
            securityContext:
              allowPrivilegeEscalation: false
              runAsUser: 0
 #         volumes:
 #           - name: ray-logs
 #             hostPath:
 #               path: "/mnt/data"
##########################################
## GPU Workers node groups
##########################################
#    - replicas: 0
#      minReplicas:  0
#      maxReplicas: {{ kernel_num_gpu_worker or 0 }}
#      groupName: gpu-group
#      rayStartParams:
#        block: 'true'
#      template:
#        spec:
#          serviceAccountName: "{{ kernel_service_account_name }}"
##          nodeSelector:
##            node.kubernetes.io/instance-type: {{ kernel_gpu_instance_type | default("g5.4xlarge")}}
#          initContainers:
#          - name: init
#            image: busybox:1.28
#            command: ['sh', '-c', "until nslookup $RAY_IP.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for K8s Service $RAY_IP; sleep 2; done"]
#          containers:
#          - name: ray-gpu-worker
#            image: {{ kernel_image }}
#            imagePullPolicy: Always
#            env:
#              - name: PIP_INDEX_URL
#                value: https://pypi.org
#            resources:
#              limits:
#                cpu: {{ kernel_gpu_worker_num_cpu_limit | default(1)}}
#                memory: {{ kernel_gpu_worker_num_memory_limit | default("1Gi")}}
#                nvidia.com/gpu: {{ kernel_gpu_worker_num_gpu | default(0)}}
#              requests:
#                cpu: {{kernel_gpu_worker_num_cpu_request | default(0)}}
#                memory: {{ kernel_gpu_worker_num_memory_request | default("512Mi")}}
#                nvidia.com/gpu:  {{ kernel_gpu_worker_num_gpu | default(0)}}
#            securityContext:
#              allowPrivilegeEscalation: false
#              runAsUser: 0
